{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "TextClassificationModelsBinary.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "J6ASGtqCZ2ep",
        "CGaGbhVEa40P",
        "xLv5qAsE1v84",
        "3CjxVY1JbUil",
        "l8vQ5mWhTbye",
        "V2GssKRALHh7"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J6ASGtqCZ2ep"
      },
      "source": [
        "# INSTALLS AND IMPORTS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r2wiOipdZfRO"
      },
      "source": [
        "!pip install flair"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KsNeSnuzZ0U_"
      },
      "source": [
        "import pandas as pd\n",
        "import zipfile\n",
        "from flair.data import Sentence, Corpus\n",
        "from sklearn.model_selection import train_test_split\n",
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, DocumentRNNEmbeddings\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from flair.trainers import ModelTrainer\n",
        "import torch\n",
        "from flair.models import TextClassifier\n",
        "from flair.embeddings import TransformerDocumentEmbeddings\n",
        "from torch.optim.adam import Adam\n",
        "from flair.embeddings import TransformerWordEmbeddings\n",
        "from sklearn.utils import resample"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGaGbhVEa40P"
      },
      "source": [
        "# LOAD DATA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hsjt-_Hja3uJ"
      },
      "source": [
        "# download data\n",
        "#!wget https://github.com/untruenews/ss2021/raw/main/data/data.csv.zip\n",
        "zf = zipfile.ZipFile(\"data.csv.zip\") \n",
        "\n",
        "# open\n",
        "df_raw = pd.read_csv(zf.open('data_out.csv')).sample(n=35000, random_state=1)\n",
        "df_raw[\"rating_alternateName_normalized\"]=df_raw[\"rating_alternateName_normalized\"].str.upper() #only making sure the labels are normalized.\n",
        "\n",
        "# taking only the english text\n",
        "df_english=df_raw[df_raw.language==\"en\"][[\"claimReview_claimReviewed\",\"rating_alternateName_normalized\"]]\n",
        "df_english.rating_alternateName_normalized.hist().plot()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xLv5qAsE1v84"
      },
      "source": [
        "# BALANCE THE DATASET"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MG5bXuNlv9Ns"
      },
      "source": [
        "# find smallest class\n",
        "df_english[\"rating_alternateName_normalized\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KlQnRj_3RkJ3"
      },
      "source": [
        "**CHOOSE BETWEEN BINARY AND MULTI CLASS CLASSIFICATION**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECG7uYvGRX65"
      },
      "source": [
        "binaryClassification = False"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPXKjwKsAizi"
      },
      "source": [
        "if binaryClassification == True:\n",
        "    # split into classes\n",
        "    df_true = df_english[df_english[\"rating_alternateName_normalized\"]==\"TRUE\"]\n",
        "    df_false = df_english[df_english[\"rating_alternateName_normalized\"]==\"FALSE\"]\n",
        "\n",
        "    # resample\n",
        "    num_samples_smallest_class = df_english[\"rating_alternateName_normalized\"].value_counts().TRUE\n",
        "\n",
        "    true_resampled = resample(df_true, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "    false_resampled = resample(df_false, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "\n",
        "    # concatenate\n",
        "    df_concatenated = pd.concat([true_resampled, false_resampled])\n",
        "\n",
        "else:\n",
        "    # split into classes\n",
        "    df_other = df_english[df_english[\"rating_alternateName_normalized\"]==\"OTHER\"]\n",
        "    df_mixture = df_english[df_english[\"rating_alternateName_normalized\"]==\"MIXTURE\"]\n",
        "    df_true = df_english[df_english[\"rating_alternateName_normalized\"]==\"TRUE\"]\n",
        "    df_false = df_english[df_english[\"rating_alternateName_normalized\"]==\"FALSE\"]\n",
        "\n",
        "    # resample\n",
        "    num_samples_smallest_class = df_english[\"rating_alternateName_normalized\"].value_counts().MIXTURE\n",
        "\n",
        "    other_resampled = resample(df_other, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "    mixture_resampled = resample(df_mixture, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "    true_resampled = resample(df_true, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "    false_resampled = resample(df_false, replace=True, n_samples=num_samples_smallest_class, random_state=123)\n",
        "\n",
        "    # concatenate\n",
        "    df_concatenated = pd.concat([other_resampled, mixture_resampled, true_resampled, false_resampled])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3CjxVY1JbUil"
      },
      "source": [
        "# PREPARE CORPUS"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ1WChmgBJJP"
      },
      "source": [
        "class Params():\n",
        "    def __init__(self, model_name, lr, epochs):\n",
        "        self.model_name = model_name\n",
        "        self.lr = lr\n",
        "        self.epochs = epochs\n",
        "\n",
        "    def show(self):\n",
        "        print(\"------------------\")\n",
        "        print(\"ARGUMENTS\")\n",
        "        print()\n",
        "        print(\"Model:\\t\\t\\t\" + self.model_name)\n",
        "        print(\"Epochs:\\t\\t\\t\" + str(self.epochs))\n",
        "        print(\"Learning Rate:\\t\\t\" + str(self.lr))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nGZt4dpzFmjS"
      },
      "source": [
        "# split the data\n",
        "X=df_concatenated[\"claimReview_claimReviewed\"]\n",
        "y=df_concatenated[\"rating_alternateName_normalized\"]\n",
        "X_train, X_test, y_train, y_test = train_test_split(list(X), list(y), test_size=0.3, random_state=32)\n",
        "\n",
        "# create the corpus\n",
        "train = [Sentence(X_train[i]).add_label(\"class\",y_train[i]) for i in range(len(X_train))]\n",
        "dev = [Sentence(X_test[i]).add_label(\"class\",y_test[i]) for i in range(len(X_test))]\n",
        "test = [Sentence(X_test[i]).add_label(\"class\",y_test[i]) for i in range(len(X_test))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LULUbF9SBg7O"
      },
      "source": [
        "# 1. get the corpus\n",
        "corpus: Corpus = Corpus(train, dev, test)\n",
        "\n",
        "# 2. create the label dictionary\n",
        "label_dict = corpus.make_label_dictionary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p86DWRnQLBY7"
      },
      "source": [
        "# TRAIN HUGGINGFACE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6qcHuyoBHdQ"
      },
      "source": [
        "# set parameters (models can be found on https://huggingface.co/models)\n",
        "# for this work the following models were used:\n",
        "#   - 'vinai/bertweet-base'\n",
        "#   - 'roberta-base'\n",
        "\n",
        "ARGS = Params(model_name='vinai/bertweet-base', lr=3e-5, epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q6tpim55A4n9"
      },
      "source": [
        "# DOCUMENT TRANSFORMER EMBEDDINGS\n",
        "document_embeddings = TransformerDocumentEmbeddings(ARGS.model_name, fine_tune=True)\n",
        "\n",
        "# create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# initialize trainer\n",
        "trainer: ModelTrainer = ModelTrainer(classifier, corpus, optimizer=Adam)\n",
        "\n",
        "# start training\n",
        "trainer.train('models/classifier',\n",
        "              learning_rate=ARGS.lr,\n",
        "              mini_batch_size=32,\n",
        "              embeddings_storage_mode='gpu',\n",
        "              max_epochs=ARGS.epochs,\n",
        "              )\n",
        "\n",
        "ARGS.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l8vQ5mWhTbye"
      },
      "source": [
        "# LOAD MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAd42lqWTay3"
      },
      "source": [
        "!pip install gdown"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lxXQ8ZCuUsyf"
      },
      "source": [
        "import gdown\n",
        "url = 'https://drive.google.com/uc?id=1-ouVsPHfIM7pscfguU_upcjZao55A02S'\n",
        "output = 'best-model.pt'\n",
        "gdown.download(url, output, quiet = False)\n",
        "model = TextClassifier.load('./best-model.pt')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXuWROh3VYVA"
      },
      "source": [
        "result, nothing  = model.evaluate(sentences=test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yvUdweZpYwJ6"
      },
      "source": [
        "print(result.detailed_results)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2GssKRALHh7"
      },
      "source": [
        "# TRAIN FLAIR EMBEDDING"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiJad2T61uc7"
      },
      "source": [
        "from flair.embeddings import WordEmbeddings, FlairEmbeddings, StackedEmbeddings\n",
        "\n",
        "# create a StackedEmbedding object that combines forward/backward flair embeddings\n",
        "embeddings = [FlairEmbeddings('news-forward'), FlairEmbeddings('news-backward')]\n",
        "\n",
        "document_embeddings = DocumentRNNEmbeddings(embeddings, hidden_size=256)\n",
        "\n",
        "# create the text classifier\n",
        "classifier = TextClassifier(document_embeddings, label_dictionary=label_dict)\n",
        "\n",
        "# initialize the text classifier trainer\n",
        "trainer = ModelTrainer(classifier, corpus)\n",
        "\n",
        "# start the training\n",
        "trainer.train('models/flair-embbeding',\n",
        "              learning_rate=0.1,\n",
        "              mini_batch_size=32,\n",
        "              anneal_factor=0.5,\n",
        "              patience=5,\n",
        "              max_epochs=15)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}